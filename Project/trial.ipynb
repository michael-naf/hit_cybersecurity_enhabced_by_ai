{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be889c99",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e0583",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09de7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap.py\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# --- TensorFlow env vars ---\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# --- Python warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*np.object.*\")\n",
    "\n",
    "# --- Logging ---\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"keras\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9981df",
   "metadata": {},
   "source": [
    "### Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2247d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tf_keras.models import Model\n",
    "from tf_keras.layers import Input, Dense\n",
    "from tf_keras.optimizers import Adam\n",
    "from tf_keras.callbacks import EarlyStopping\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc0f4e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d09080",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3994142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Encode labels\n",
    "# -----------------------------\n",
    "def encode_labels(y, label_map={'benign':0, 'suspicious':1}):\n",
    "    \"\"\"\n",
    "    Encode string labels to numeric\n",
    "    \"\"\"\n",
    "    return y.map(label_map)\n",
    "\n",
    "# -----------------------------\n",
    "# Timestamp processing (fully cyclic)\n",
    "# -----------------------------\n",
    "def process_timestamp(X, timestamp_col):\n",
    "    \"\"\"\n",
    "    Convert timestamp to datetime and extract numeric + cyclic features.\n",
    "    Fully robust to ISO format (YYYY-MM-DD HH:MM:SS) and other formats.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Convert to datetime\n",
    "    # -----------------------------\n",
    "    X[timestamp_col] = pd.to_datetime(X[timestamp_col], dayfirst=True, errors='coerce')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Basic numeric features\n",
    "    # -----------------------------\n",
    "    X['year'] = X[timestamp_col].dt.year\n",
    "    X['month'] = X[timestamp_col].dt.month\n",
    "    X['day'] = X[timestamp_col].dt.day\n",
    "    X['hour'] = X[timestamp_col].dt.hour\n",
    "    X['minute'] = X[timestamp_col].dt.minute\n",
    "    X['dayofweek'] = X[timestamp_col].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    # Shift to Sunday=0\n",
    "    X['dayofweek'] = (X['dayofweek'] + 1) % 7\n",
    "    X['is_weekend'] = X['dayofweek'].isin([0,6]).astype(int)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Cyclic encoding\n",
    "    # -----------------------------\n",
    "    # Hour\n",
    "    X['hour_sin'] = np.sin(2 * np.pi * X['hour']/24)\n",
    "    X['hour_cos'] = np.cos(2 * np.pi * X['hour']/24)\n",
    "    # Minute\n",
    "    X['minute_sin'] = np.sin(2 * np.pi * X['minute']/60)\n",
    "    X['minute_cos'] = np.cos(2 * np.pi * X['minute']/60)\n",
    "    # Day of month (manual month lengths)\n",
    "    month_days = {1:31,2:28,3:31,4:30,5:31,6:30,\n",
    "                  7:31,8:31,9:30,10:31,11:30,12:31}\n",
    "    days_in_month = X['month'].map(month_days)\n",
    "    X['day_sin'] = np.sin(2 * np.pi * X['day'] / days_in_month)\n",
    "    X['day_cos'] = np.cos(2 * np.pi * X['day'] / days_in_month)\n",
    "    # Day of week\n",
    "    X['dow_sin'] = np.sin(2 * np.pi * X['dayofweek']/7)\n",
    "    X['dow_cos'] = np.cos(2 * np.pi * X['dayofweek']/7)\n",
    "    # Month\n",
    "    X['month_sin'] = np.sin(2 * np.pi * X['month']/12)\n",
    "    X['month_cos'] = np.cos(2 * np.pi * X['month']/12)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Drop raw columns\n",
    "    # -----------------------------\n",
    "    drop_cols = [timestamp_col, 'hour','minute','day','dayofweek','month']\n",
    "    X = X.drop(columns=drop_cols, errors='ignore')  # safer in case some columns missing\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Handle missing values\n",
    "# -----------------------------\n",
    "def handle_missing_values(X, numeric_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Fill missing values in numeric and categorical columns\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "    X[categorical_cols] = X[categorical_cols].fillna('Unknown')\n",
    "    return X\n",
    "\n",
    "# -----------------------------\n",
    "# Handle outliers\n",
    "# -----------------------------\n",
    "def handle_outliers(X, numeric_cols):\n",
    "    \"\"\"\n",
    "    Clip numeric features to remove extreme outliers (IQR method)\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    Q1 = X[numeric_cols].quantile(0.25)\n",
    "    Q3 = X[numeric_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    X[numeric_cols] = X[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n",
    "    return X\n",
    "\n",
    "# -----------------------------\n",
    "# Remove highly correlated features\n",
    "# -----------------------------\n",
    "def remove_highly_correlated(X, numeric_cols, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove highly correlated numeric features\n",
    "    Returns: reduced X, updated numeric_cols, list of removed features\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    corr_matrix = X[numeric_cols].corr().abs()\n",
    "    upper_tri = corr_matrix.where(~np.tril(np.ones(corr_matrix.shape)).astype(bool))\n",
    "    to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    numeric_cols = [col for col in numeric_cols if col not in to_drop]\n",
    "    return X, numeric_cols, to_drop\n",
    "\n",
    "# -----------------------------\n",
    "# Embed command_text semantically\n",
    "# -----------------------------\n",
    "def embed_command_text(X, text_col='command_text', model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Convert command_text into semantic embeddings using a pretrained sentence transformer\n",
    "    Returns a DataFrame of embeddings\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "    embeddings = model.encode(X[text_col].tolist(), show_progress_bar=True)\n",
    "    X_emb = pd.DataFrame(embeddings, columns=[f\"{text_col}_embed_{i}\" for i in range(embeddings.shape[1])])\n",
    "    X_emb.index = X.index\n",
    "    return X_emb\n",
    "\n",
    "# -----------------------------\n",
    "# Full preprocessing pipeline\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "X_structured, numeric_cols, y, X_text_emb = preprocess_data(\n",
    "    df,\n",
    "    timestamp_col=\"timestamp\",\n",
    "    label_col=\"is_anomaly\",\n",
    "    text_col=\"command_text\",\n",
    "    numeric_cols=None,       # auto-detect\n",
    "    categorical_cols=None    # auto-detect\n",
    ")\n",
    "\"\"\"\n",
    "def preprocess_data(\n",
    "    X,\n",
    "    timestamp_col=None,\n",
    "    text_col=None,\n",
    "    label_col=None,\n",
    "    numeric_cols=None,\n",
    "    categorical_cols=None\n",
    "):\n",
    "    X = X.copy()\n",
    "\n",
    "    # --------------------------\n",
    "    # Auto-detect numeric/categorical if not provided\n",
    "    # --------------------------\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "        # remove label/text/timestamp if present\n",
    "        categorical_cols = [col for col in categorical_cols if col not in [label_col, text_col, timestamp_col]]\n",
    "\n",
    "    # --------------------------\n",
    "    # Process timestamp\n",
    "    # --------------------------\n",
    "    if timestamp_col and timestamp_col in X.columns:\n",
    "        X = process_timestamp(X, timestamp_col)\n",
    "        # Add new time features to numeric_cols\n",
    "        new_time_features = [\n",
    "            'hour_sin','hour_cos',\n",
    "            'minute_sin','minute_cos',\n",
    "            'day_sin','day_cos',\n",
    "            'dow_sin','dow_cos',\n",
    "            'month_sin','month_cos'\n",
    "        ]\n",
    "        numeric_cols += new_time_features\n",
    "\n",
    "    # --------------------------\n",
    "    # Handle missing values\n",
    "    # --------------------------\n",
    "    X = handle_missing_values(X, numeric_cols, categorical_cols)\n",
    "\n",
    "    # --------------------------\n",
    "    # Handle outliers\n",
    "    # --------------------------\n",
    "    X = handle_outliers(X, numeric_cols)\n",
    "\n",
    "    # --------------------------\n",
    "    # Remove highly correlated features\n",
    "    # --------------------------\n",
    "    X, numeric_cols, dropped = remove_highly_correlated(X, numeric_cols)\n",
    "\n",
    "    # --------------------------\n",
    "    # Encode labels\n",
    "    # --------------------------\n",
    "    y_encoded = None\n",
    "    if label_col and label_col in X.columns:\n",
    "        y_encoded = encode_labels(X[label_col])\n",
    "        X = X.drop(columns=[label_col])\n",
    "\n",
    "    # --------------------------\n",
    "    # Embed command_text\n",
    "    # --------------------------\n",
    "    X_text_emb = None\n",
    "    if text_col and text_col in X.columns:\n",
    "        X_text_emb = embed_command_text(X, text_col=text_col)\n",
    "\n",
    "    return X, numeric_cols, y_encoded, X_text_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9e69e",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0e0a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Isolation Forest\n",
    "# -----------------------------\n",
    "def features_for_isolation_forest(X_structured, numeric_cols):\n",
    "    \"\"\"\n",
    "    Isolation Forest:\n",
    "    - Numeric features only\n",
    "    - No scaling required\n",
    "    \"\"\"\n",
    "    return X_structured[numeric_cols]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SVM\n",
    "# -----------------------------\n",
    "def features_for_svm(X_structured, numeric_cols):\n",
    "    \"\"\"\n",
    "    SVM:\n",
    "    - Numeric features\n",
    "    - Requires scaling\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_num = X_structured[numeric_cols]\n",
    "    X_scaled = scaler.fit_transform(X_num)\n",
    "    return pd.DataFrame(X_scaled, columns=numeric_cols, index=X_structured.index)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Autoencoder\n",
    "# -----------------------------\n",
    "def features_for_autoencoder(X_structured, numeric_cols, X_text_emb=None, use_text=True):\n",
    "    \"\"\"\n",
    "    Autoencoder:\n",
    "    - Numeric features\n",
    "    - Optionally semantic embeddings\n",
    "    \"\"\"\n",
    "    X_num = X_structured[numeric_cols]\n",
    "\n",
    "    if use_text and X_text_emb is not None:\n",
    "        return pd.concat([X_num, X_text_emb], axis=1)\n",
    "\n",
    "    return X_num\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Meta-Agent Input\n",
    "# -----------------------------\n",
    "def features_for_meta_agent(model_outputs):\n",
    "    \"\"\"\n",
    "    Meta-agent does NOT see raw data.\n",
    "    It sees model-level outputs like:\n",
    "    - anomaly scores\n",
    "    - reconstruction errors\n",
    "    - decision margins\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(model_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36d986",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415cdf6",
   "metadata": {},
   "source": [
    "### Base Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82eeee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all model agents.\n",
    "    Enforces a common interface across agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, X):\n",
    "        \"\"\"\n",
    "        Produce anomaly scores.\n",
    "        Higher score = more anomalous.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, threshold=None):\n",
    "        \"\"\"\n",
    "        Optional binary prediction from anomaly scores.\n",
    "        \"\"\"\n",
    "        scores = self.score(X)\n",
    "\n",
    "        if threshold is None:\n",
    "            return scores\n",
    "\n",
    "        return (scores > threshold).astype(int)\n",
    "\n",
    "    def get_name(self):\n",
    "        \"\"\"\n",
    "        Return agent name.\n",
    "        \"\"\"\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afb3ac",
   "metadata": {},
   "source": [
    "### IF Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7108f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForestAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Isolation Forest agent for anomaly detection.\n",
    "    Higher score = more anomalous.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"IsolationForest\",\n",
    "        n_estimators=100,\n",
    "        max_samples=\"auto\",\n",
    "        contamination=0.05,\n",
    "        random_state=42\n",
    "    ):\n",
    "        super().__init__(name)\n",
    "\n",
    "        self.model = IsolationForest(\n",
    "            n_estimators=n_estimators,\n",
    "            max_samples=max_samples,\n",
    "            contamination=contamination,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the Isolation Forest on numeric features only.\n",
    "        \"\"\"\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def score(self, X):\n",
    "        \"\"\"\n",
    "        Return anomaly scores.\n",
    "        Higher = more anomalous.\n",
    "        \"\"\"\n",
    "\n",
    "        # sklearn: decision_function\n",
    "        #   higher = more normal\n",
    "        #   lower = more anomalous\n",
    "        scores = self.model.decision_function(X)\n",
    "\n",
    "        # Flip sign: now higher = more anomalous\n",
    "        return -scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2261a9",
   "metadata": {},
   "source": [
    "### AE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e878cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Autoencoder-based anomaly detection agent.\n",
    "    Anomaly score = reconstruction error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"Autoencoder\",\n",
    "        input_dim=None,\n",
    "        hidden_dims=(64, 32),\n",
    "        latent_dim=16,\n",
    "        learning_rate=1e-3,\n",
    "        epochs=50,\n",
    "        batch_size=32\n",
    "    ):\n",
    "        super().__init__(name)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build symmetric autoencoder.\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "\n",
    "        x = inputs\n",
    "        for dim in self.hidden_dims:\n",
    "            x = Dense(dim, activation=\"relu\")(x)\n",
    "\n",
    "        latent = Dense(self.latent_dim, activation=\"relu\")(x)\n",
    "\n",
    "        x = latent\n",
    "        for dim in reversed(self.hidden_dims):\n",
    "            x = Dense(dim, activation=\"relu\")(x)\n",
    "\n",
    "        outputs = Dense(self.input_dim, activation=\"linear\")(x)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate),\n",
    "            loss=\"mse\"\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train autoencoder on (mostly) normal data.\n",
    "        \"\"\"\n",
    "        if self.input_dim is None:\n",
    "            self.input_dim = X.shape[1]\n",
    "\n",
    "        self.model = self._build_model()\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X,\n",
    "            X,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    def score(self, X):\n",
    "        \"\"\"\n",
    "        Return anomaly scores.\n",
    "        Higher = more anomalous.\n",
    "        \"\"\"\n",
    "        reconstructions = self.model.predict(X, verbose=0)\n",
    "\n",
    "        # Mean Squared Error per sample\n",
    "        reconstruction_error = np.mean(\n",
    "            np.square(X - reconstructions),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return reconstruction_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc399d2",
   "metadata": {},
   "source": [
    "### SVM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af6edf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    One-Class SVM agent for anomaly detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"OneClassSVM\",\n",
    "        kernel=\"rbf\",\n",
    "        nu=0.05,\n",
    "        gamma=\"scale\"\n",
    "    ):\n",
    "        super().__init__(name)\n",
    "\n",
    "        self.model = OneClassSVM(\n",
    "            kernel=kernel,\n",
    "            nu=nu,\n",
    "            gamma=gamma\n",
    "        )\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the One-Class SVM.\n",
    "        X must be numeric features only.\n",
    "        \"\"\"\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def score(self, X):\n",
    "        \"\"\"\n",
    "        Return anomaly scores.\n",
    "        Higher score = more anomalous.\n",
    "        \"\"\"\n",
    "\n",
    "        # decision_function:\n",
    "        #   positive → inlier\n",
    "        #   negative → outlier\n",
    "        scores = self.model.decision_function(X)\n",
    "\n",
    "        # Flip sign so:\n",
    "        #   higher = more anomalous\n",
    "        return -scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e077e7e",
   "metadata": {},
   "source": [
    "### META Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91518781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Meta-agent that aggregates multiple anomaly detection agents.\n",
    "    Uses ensemble scoring (mean, weighted, or custom) to produce\n",
    "    a final anomaly score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agents, name=\"MetaAgent\", weights=None):\n",
    "        \"\"\"\n",
    "        :param agents: list of BaseAgent instances\n",
    "        :param weights: optional list of weights for each agent\n",
    "        \"\"\"\n",
    "        super().__init__(name)\n",
    "        self.agents = agents\n",
    "\n",
    "        if weights is None:\n",
    "            # Equal weight by default\n",
    "            self.weights = np.ones(len(agents)) / len(agents)\n",
    "        else:\n",
    "            assert len(weights) == len(agents), \"weights length must match number of agents\"\n",
    "            self.weights = np.array(weights) / np.sum(weights)  # normalize to sum=1\n",
    "\n",
    "    def fit(self, X_dict):\n",
    "        \"\"\"\n",
    "        Fit all agents.\n",
    "        :param X_dict: dict of {agent_name: X_features_for_agent}\n",
    "        \"\"\"\n",
    "        for agent in self.agents:\n",
    "            X_agent = X_dict[agent.get_name()]\n",
    "            agent.fit(X_agent)\n",
    "\n",
    "    def score(self, X_dict):\n",
    "        \"\"\"\n",
    "        Return ensemble anomaly scores.\n",
    "        :param X_dict: dict of {agent_name: X_features_for_agent}\n",
    "        :return: np.array of final anomaly scores (higher = more anomalous)\n",
    "        \"\"\"\n",
    "        all_scores = []\n",
    "\n",
    "        for agent in self.agents:\n",
    "            X_agent = X_dict[agent.get_name()]\n",
    "            scores = agent.score(X_agent)\n",
    "            all_scores.append(scores)\n",
    "\n",
    "        all_scores = np.array(all_scores)  # shape: (num_agents, num_samples)\n",
    "\n",
    "        # Weighted sum across agents\n",
    "        final_scores = np.dot(self.weights, all_scores)\n",
    "\n",
    "        return final_scores\n",
    "\n",
    "    def predict(self, X_dict, threshold=None):\n",
    "        \"\"\"\n",
    "        Optional binary prediction based on ensemble score.\n",
    "        \"\"\"\n",
    "        final_scores = self.score(X_dict)\n",
    "\n",
    "        if threshold is None:\n",
    "            return final_scores\n",
    "\n",
    "        return (final_scores > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78445966",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b14527f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 19.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns used: ['command_length', 'num_arguments', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'day_sin', 'day_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
      "=== Evaluation Metrics ===\n",
      "Accuracy:  0.9650\n",
      "Precision: 0.6600\n",
      "Recall:    0.6471\n",
      "F1-score:  0.6535\n",
      "ROC AUC:   0.8146\n",
      "Confusion Matrix:\n",
      "[[932  17]\n",
      " [ 18  33]]\n",
      "Anomaly detection complete. Results saved to anomaly_results.csv\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate anomaly detection predictions\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auc = None  # במקרה שאין מספיק מחלקות\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(\"=== Evaluation Metrics ===\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC AUC:   {auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc, \"confusion_matrix\": cm}\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --------------------------\n",
    "    # 1. Load your dataset\n",
    "    # --------------------------\n",
    "    df = pd.read_csv(\"data/enterprise_A.csv\")\n",
    "\n",
    "    # --------------------------\n",
    "    # 2. Preprocess data\n",
    "    # --------------------------\n",
    "    # Preprocess numeric, categorical, timestamp, label, and embed text\n",
    "    # Updated: unpack all 4 outputs\n",
    "    X_structured, numeric_cols, y, X_text_emb = preprocess_data(\n",
    "        df,\n",
    "        timestamp_col=\"timestamp\",\n",
    "        label_col=\"is_anomaly\",\n",
    "        text_col=\"command_text\",\n",
    "        numeric_cols=None,       # auto-detect\n",
    "        categorical_cols=None    # auto-detect\n",
    "    )\n",
    "\n",
    "    # --------------------------\n",
    "    # 3. Standardize numeric features\n",
    "    # --------------------------\n",
    "    print(\"Numeric columns used:\", numeric_cols)\n",
    "    scaler = StandardScaler()\n",
    "    X_structured[numeric_cols] = scaler.fit_transform(X_structured[numeric_cols])\n",
    "\n",
    "    # --------------------------\n",
    "    # 4. Prepare features for each agent\n",
    "    # --------------------------\n",
    "    X_if = features_for_isolation_forest(X_structured, numeric_cols)\n",
    "    X_svm = features_for_svm(X_structured, numeric_cols)\n",
    "    X_ae = features_for_autoencoder(X_structured, numeric_cols, X_text_emb=X_text_emb)\n",
    "\n",
    "    # Build feature dict for meta-agent\n",
    "    X_dict = {\n",
    "        \"IsolationForest\": X_if,\n",
    "        \"OneClassSVM\": X_svm,\n",
    "        \"Autoencoder\": X_ae\n",
    "    }\n",
    "\n",
    "    # --------------------------\n",
    "    # 5. Initialize agents\n",
    "    # --------------------------\n",
    "    if_agent = IsolationForestAgent(contamination=0.05)\n",
    "    svm_agent = SVMAgent(nu=0.05)\n",
    "    ae_agent = AutoencoderAgent(input_dim=X_ae.shape[1], epochs=50, latent_dim=16)\n",
    "\n",
    "    agents = [if_agent, svm_agent, ae_agent]\n",
    "\n",
    "    # --------------------------\n",
    "    # 6. Initialize meta-agent\n",
    "    # --------------------------\n",
    "    meta_agent = MetaAgent(agents, weights=[0.3, 0.3, 0.4])\n",
    "\n",
    "    # --------------------------\n",
    "    # 7. Fit all agents\n",
    "    # --------------------------\n",
    "    meta_agent.fit(X_dict)  # pass y if needed by agents\n",
    "\n",
    "    # --------------------------\n",
    "    # 8. Compute final anomaly scores\n",
    "    # --------------------------\n",
    "    final_scores = meta_agent.score(X_dict)\n",
    "\n",
    "    # --------------------------\n",
    "    # 9. Optional: threshold for binary anomalies\n",
    "    # --------------------------\n",
    "    threshold = pd.Series(final_scores).quantile(0.95)  # top 5% = anomalies\n",
    "    preds = (final_scores > threshold).astype(int)\n",
    "    metrics = evaluate_model(y, preds)\n",
    "    # --------------------------\n",
    "    # 10. Save results\n",
    "    # --------------------------\n",
    "    results = df.copy()\n",
    "    results[\"anomaly_score\"] = final_scores\n",
    "    results[\"predicted_anomaly\"] = preds\n",
    "\n",
    "    results.to_csv(\"anomaly_results.csv\", index=False)\n",
    "    print(\"Anomaly detection complete. Results saved to anomaly_results.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybersecurity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
