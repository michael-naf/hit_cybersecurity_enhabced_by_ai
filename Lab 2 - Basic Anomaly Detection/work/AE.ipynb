{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f963c4a-5562-472f-b146-29a1ee3849e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a410f99e-b5f4-434c-8598-3fbd5662a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (125973, 43)\n",
      "\n",
      "First rows:\n",
      "   duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp  ftp_data   SF        491          0     0   \n",
      "1         0           udp     other   SF        146          0     0   \n",
      "2         0           tcp   private   S0          0          0     0   \n",
      "3         0           tcp      http   SF        232       8153     0   \n",
      "4         0           tcp      http   SF        199        420     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
      "0               0       0    0  ...                    0.17   \n",
      "1               0       0    0  ...                    0.00   \n",
      "2               0       0    0  ...                    0.10   \n",
      "3               0       0    0  ...                    1.00   \n",
      "4               0       0    0  ...                    1.00   \n",
      "\n",
      "   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
      "0                    0.03                         0.17   \n",
      "1                    0.60                         0.88   \n",
      "2                    0.05                         0.00   \n",
      "3                    0.00                         0.03   \n",
      "4                    0.00                         0.00   \n",
      "\n",
      "   dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
      "0                         0.00                  0.00   \n",
      "1                         0.00                  0.00   \n",
      "2                         0.00                  1.00   \n",
      "3                         0.04                  0.03   \n",
      "4                         0.00                  0.00   \n",
      "\n",
      "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
      "0                      0.00                  0.05                      0.00   \n",
      "1                      0.00                  0.00                      0.00   \n",
      "2                      1.00                  0.00                      0.00   \n",
      "3                      0.01                  0.00                      0.01   \n",
      "4                      0.00                  0.00                      0.00   \n",
      "\n",
      "     label  difficulty  \n",
      "0   normal          20  \n",
      "1   normal          15  \n",
      "2  neptune          19  \n",
      "3   normal          21  \n",
      "4   normal          21  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0=все, 1=INFO, 2=WARNING, 3=ERROR\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ========== 1. DATA LOADING ==========\n",
    "# Column names for NSL-KDD\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', \n",
    "    'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "    'num_failed_logins', 'logged_in', 'num_compromised', \n",
    "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
    "    'is_host_login', 'is_guest_login', 'count', 'srv_count',\n",
    "    'serror_rate', 'srv_serror_rate', 'rerror_rate', \n",
    "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
    "    'label', 'difficulty'\n",
    "]\n",
    "\n",
    "# Load NSL_KDD dataset from github\n",
    "# https://www.unb.ca/cic/datasets/nsl.html\n",
    "url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt\"\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f8809a-d57f-4048-b41d-a077197648cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric features: 38\n",
      "Categorical features: 3\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. PREPROCESSING ==========\n",
    "# Remove difficulty column (if exists)\n",
    "if 'difficulty' in df.columns:\n",
    "    df = df.drop('difficulty', axis=1)\n",
    "\n",
    "# Create binary label: normal = 0, attack = 1\n",
    "df['is_attack'] = (df['label'] != 'normal').astype(int)\n",
    "labels = df['is_attack'].values\n",
    "df = df.drop('label', axis=1)\n",
    "\n",
    "# Split into categorical and numeric features\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numeric_cols = [col for col in df.columns if col not in categorical_cols + ['is_attack']]\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "# Encode categorical features\n",
    "df_encoded = df.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separate labels\n",
    "X = df_encoded.drop('is_attack', axis=1).values\n",
    "y = df_encoded['is_attack'].values\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9ed652-78ab-4ecc-b20d-e75a98eb11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training examples (normal only): 53874\n",
      "Test examples (normal + attacks): 25195\n"
     ]
    }
   ],
   "source": [
    "# ========== 3. TRAINING AUTOENCODER ON NORMAL TRAFFIC ONLY ==========\n",
    "# Key point: autoencoder learns from normal data only\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "print(f\"\\nTraining examples (normal only): {len(X_train_normal)}\")\n",
    "print(f\"Test examples (normal + attacks): {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7680848a-c7f1-416f-bb9d-45472f5bdead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoder architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 18:18:44.760678: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"autoencoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"autoencoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ latent_space (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">462</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reconstruction (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,353</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ latent_space (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m462\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reconstruction (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)             │         \u001b[38;5;34m1,353\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,639</span> (14.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,639\u001b[0m (14.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,639</span> (14.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,639\u001b[0m (14.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========== 4. AUTOENCODER ARCHITECTURE WITH NAMED LAYERS ==========\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14  # Latent representation size\n",
    "\n",
    "# Build autoencoder with named layers\n",
    "encoder_input = layers.Input(shape=(input_dim,), name='input')\n",
    "encoded = layers.Dense(32, activation='relu', name='encoder_dense')(encoder_input)\n",
    "encoded = layers.Dense(encoding_dim, activation='relu', name='latent_space')(encoded)\n",
    "\n",
    "decoded = layers.Dense(32, activation='relu', name='decoder_dense')(encoded)\n",
    "decoded = layers.Dense(input_dim, activation='linear', name='reconstruction')(decoded)\n",
    "\n",
    "# Create autoencoder model\n",
    "autoencoder = keras.Model(encoder_input, decoded, name='autoencoder')\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"\\nAutoencoder architecture:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ae9d7-1469-4ebc-b5d0-4120e9bcf097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7374 - val_loss: 0.4547\n",
      "Epoch 2/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4677 - val_loss: 0.3092\n",
      "Epoch 3/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3615 - val_loss: 0.2492\n",
      "Epoch 4/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3128 - val_loss: 0.2162\n",
      "Epoch 5/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2670 - val_loss: 0.1919\n",
      "Epoch 6/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2275 - val_loss: 0.1688\n",
      "Epoch 7/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1950 - val_loss: 0.1523\n",
      "Epoch 8/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1593 - val_loss: 0.1497\n",
      "Epoch 9/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1314 - val_loss: 0.1264\n",
      "Epoch 10/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1129 - val_loss: 0.1165\n",
      "Epoch 11/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0928 - val_loss: 0.1108\n",
      "Epoch 12/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0787 - val_loss: 0.1087\n",
      "Epoch 13/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0739 - val_loss: 0.1205\n",
      "Epoch 14/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0770 - val_loss: 0.1020\n",
      "Epoch 15/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0766 - val_loss: 0.0960\n",
      "Epoch 16/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0657 - val_loss: 0.0939\n",
      "Epoch 17/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0605 - val_loss: 0.0911\n",
      "Epoch 18/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0566 - val_loss: 0.0901\n",
      "Epoch 19/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0514 - val_loss: 0.0916\n",
      "Epoch 20/50\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0554 - val_loss: 0.0883\n",
      "Epoch 21/50\n",
      "\u001b[1m  1/190\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0334"
     ]
    }
   ],
   "source": [
    "# ========== 5. TRAINING ==========\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal, X_train_normal,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd11f51-addf-4ba7-bad1-5eed6eedfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 6. EXTRACT EMBEDDINGS AND RECONSTRUCTIONS ==========\n",
    "# Create model to extract latent representations (embeddings)\n",
    "latent_model = keras.Model(\n",
    "    inputs=autoencoder.input,\n",
    "    outputs=autoencoder.get_layer('latent_space').output\n",
    ")\n",
    "\n",
    "# Get embeddings for train and test sets\n",
    "train_embeddings = latent_model.predict(X_train)\n",
    "test_embeddings = latent_model.predict(X_test)\n",
    "\n",
    "# Get reconstructions\n",
    "X_train_pred = autoencoder.predict(X_train)\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {test_embeddings.shape}\")\n",
    "print(f\"Original input shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f82d74-3a25-444a-adcc-356205fbbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. ANOMALY DETECTION ==========\n",
    "# Calculate reconstruction error\n",
    "train_mse = np.mean(np.power(X_train - X_train_pred, 2), axis=1)\n",
    "test_mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)\n",
    "\n",
    "# Define threshold (e.g., 95th percentile on training set)\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "print(f\"\\nAnomaly detection threshold: {threshold:.4f}\")\n",
    "\n",
    "# Predict anomalies\n",
    "y_pred = (test_mse > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841a42c-ab0d-4f8d-bd14-7e048262fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8. EVALUATION ==========\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Attack']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151a26e-3f91-4832-9d45-dfeba02527c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 9. VISUALIZATION ==========\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Loss during training\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "ax1.plot(history.history['loss'], label='Train Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Autoencoder Training')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Distribution of reconstruction error\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "ax2.hist(train_mse[y_train == 0], bins=50, alpha=0.6, label='Normal (train)', density=True)\n",
    "ax2.hist(test_mse[y_test == 0], bins=50, alpha=0.6, label='Normal (test)', density=True)\n",
    "ax2.hist(test_mse[y_test == 1], bins=50, alpha=0.6, label='Attack (test)', density=True)\n",
    "ax2.axvline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.2f}')\n",
    "ax2.set_xlabel('Reconstruction Error (MSE)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution of Reconstruction Error')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, min(threshold * 3, test_mse.max()))\n",
    "\n",
    "# Plot 3: Reconstruction error for each sample\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "ax3.scatter(range(len(test_mse)), test_mse, c=y_test, cmap='coolwarm', alpha=0.5, s=1)\n",
    "ax3.axhline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.2f}')\n",
    "ax3.set_xlabel('Sample Index')\n",
    "ax3.set_ylabel('Reconstruction Error')\n",
    "ax3.set_title('Reconstruction Error on Test Set')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# Plot 4-5: Latent space visualization (2D projection using PCA)\n",
    "# Reduce embeddings to 2D for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "test_embeddings_2d = pca.fit_transform(test_embeddings)\n",
    "train_embeddings_2d = pca.transform(train_embeddings)\n",
    "\n",
    "# Plot 4: Latent space colored by true labels\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "scatter = ax4.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], \n",
    "                      c=y_test, cmap='coolwarm', alpha=0.6, s=10)\n",
    "ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax4.set_title('Latent Space (True Labels)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax4)\n",
    "cbar.set_label('0=Normal, 1=Attack')\n",
    "\n",
    "# Plot 5: Latent space colored by predicted labels\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "scatter = ax5.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], \n",
    "                      c=y_pred, cmap='coolwarm', alpha=0.6, s=10)\n",
    "ax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax5.set_title('Latent Space (Predicted Labels)')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax5)\n",
    "cbar.set_label('0=Normal, 1=Attack')\n",
    "\n",
    "# Plot 6: Latent space colored by reconstruction error\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "scatter = ax6.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], \n",
    "                      c=test_mse, cmap='viridis', alpha=0.6, s=10)\n",
    "ax6.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax6.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax6.set_title('Latent Space (Reconstruction Error)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax6)\n",
    "cbar.set_label('MSE')\n",
    "\n",
    "# Plot 7-9: Example reconstructions\n",
    "n_examples = 3\n",
    "indices = np.random.choice(len(X_test), n_examples)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    ax = plt.subplot(3, 3, 7 + i)\n",
    "    original = X_test[idx]\n",
    "    reconstructed = X_test_pred[idx]\n",
    "    error = test_mse[idx]\n",
    "    label = \"Attack\" if y_test[idx] == 1 else \"Normal\"\n",
    "    \n",
    "    ax.plot(original, label='Original', alpha=0.7, linewidth=1)\n",
    "    ax.plot(reconstructed, label='Reconstructed', alpha=0.7, linewidth=1)\n",
    "    ax.set_title(f'{label} | MSE: {error:.4f} | Pred: {\"Attack\" if error > threshold else \"Normal\"}', \n",
    "                 fontsize=9)\n",
    "    ax.set_xlabel('Feature Index')\n",
    "    ax.set_ylabel('Value (normalized)')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca827f5d-fd11-4950-8213-398df8d4d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 10. DETAILED LATENT SPACE ANALYSIS ==========\n",
    "# Create a separate detailed plot for latent space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot with larger points and better separation\n",
    "ax1 = axes[0]\n",
    "# Plot normal samples first (so they're in background)\n",
    "normal_mask = y_test == 0\n",
    "attack_mask = y_test == 1\n",
    "\n",
    "ax1.scatter(test_embeddings_2d[normal_mask, 0], test_embeddings_2d[normal_mask, 1], \n",
    "           c='blue', alpha=0.4, s=20, label='Normal', edgecolors='none')\n",
    "ax1.scatter(test_embeddings_2d[attack_mask, 0], test_embeddings_2d[attack_mask, 1], \n",
    "           c='red', alpha=0.6, s=20, label='Attack', edgecolors='none')\n",
    "ax1.set_xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "ax1.set_ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "ax1.set_title('Latent Space Representation (2D PCA Projection)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot with contour showing density\n",
    "ax2 = axes[1]\n",
    "scatter = ax2.scatter(test_embeddings_2d[:, 0], test_embeddings_2d[:, 1], \n",
    "                     c=test_mse, cmap='RdYlGn_r', alpha=0.6, s=20, edgecolors='none')\n",
    "ax2.set_xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "ax2.set_ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "ax2.set_title('Latent Space by Reconstruction Error', fontsize=14, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Reconstruction Error (MSE)', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438a5d5-ff55-4cc5-83c9-5361fb9e10a7",
   "metadata": {},
   "source": [
    "## ========== HOW AUTOENCODER WORKS ==========\n",
    "\n",
    "1. TRAINING: The autoencoder is trained ONLY on normal traffic.\n",
    "   It learns to compress (encoder) and reconstruct (decoder) normal data.\n",
    "\n",
    "2. LATENT REPRESENTATION (EMBEDDINGS): \n",
    "   - The encoder compresses {input_dim} features into {encoding_dim} numbers\n",
    "   - This is accessed via the 'latent_space' layer\n",
    "   - This \"compressed code\" contains key information about normal traffic\n",
    "   - We can extract it: latent_model.predict(data)\n",
    "\n",
    "3. RECONSTRUCTION:\n",
    "   - The decoder tries to reconstruct the original input from the embeddings\n",
    "   - This is the final output of the autoencoder\n",
    "   - Good reconstruction = low MSE = likely normal traffic\n",
    "\n",
    "4. ANOMALY DETECTION: When we feed an attack into the autoencoder:\n",
    "   - It tries to reconstruct the data as if it were normal traffic\n",
    "   - It does poorly → high reconstruction error (MSE)\n",
    "   - If MSE > threshold → it's an anomaly!\n",
    "\n",
    "5. LATENT SPACE VISUALIZATION:\n",
    "   - We use PCA to project {encoding_dim}D embeddings to 2D\n",
    "   - You can see that attacks (red) and normal traffic (blue) cluster differently\n",
    "   - The autoencoder learned to separate them in the latent space!\n",
    "\n",
    "6. THRESHOLD: We set the threshold at the 95th percentile of normal traffic.\n",
    "   This means: 5% of normal examples will be falsely marked as attacks (FPR=5%).\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
